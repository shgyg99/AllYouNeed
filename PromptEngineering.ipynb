{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z2YfRf3sklax"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown, HTML"
      ],
      "metadata": {
        "id": "TlgQnvrDkxYF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "NWa1b2zBlDqr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flash = genai.GenerativeModel()\n",
        "response = flash.generate_content('Explain AI to me like Im a kid.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "_yrNqr8olaps",
        "outputId": "f60c6162-a3e5-4576-a950-24bb8f5dc84d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n",
            "\n",
            "AI is kind of like that super smart puppy, but instead of tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or sentences in different languages – and it learns to recognize patterns.\n",
            "\n",
            "So, if you show it a new picture of a cat, it can say \"That's a cat!\" because it learned from all the other cat pictures it saw before.  Or if you ask it to translate \"Hello\" into Spanish, it can do that because it learned how words translate from all the information it was given.\n",
            "\n",
            "The more information the AI gets, the smarter it gets at doing things like recognizing pictures, translating languages, playing games, and even writing stories!  It's like a really fast learner that never gets tired.  But it's important to remember, it's still learning and sometimes it makes mistakes, just like your puppy might sometimes forget its tricks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "wyNlEouXle7N",
        "outputId": "2c2f3741-6643-4029-d92a-c7bf2b1e15ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n\nAI is kind of like that super smart puppy, but instead of tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or sentences in different languages – and it learns to recognize patterns.\n\nSo, if you show it a new picture of a cat, it can say \"That's a cat!\" because it learned from all the other cat pictures it saw before.  Or if you ask it to translate \"Hello\" into Spanish, it can do that because it learned how words translate from all the information it was given.\n\nThe more information the AI gets, the smarter it gets at doing things like recognizing pictures, translating languages, playing games, and even writing stories!  It's like a really fast learner that never gets tired.  But it's important to remember, it's still learning and sometimes it makes mistakes, just like your puppy might sometimes forget its tricks.\n"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = flash.start_chat(history=[])\n",
        "response = chat.send_message('Hello! My name is Shaghayegh.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sXYW3DfimkPk",
        "outputId": "ec1fe396-97dd-450b-93f6-358539a08046"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Shaghayegh! It's nice to meet you.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('What is my name?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6Xgv8SP0m3ea",
        "outputId": "bbe85a5b-ac8d-47a3-f2c7-4d3dcff020cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Shaghayegh.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VUgXnDmFm-ST",
        "outputId": "a491d4c8-693e-4965-e210-8dbb0be7b0ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
            "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
            "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
            "                   'version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-exp-0801',\n",
            "      base_model_id='',\n",
            "      version='exp-0801',\n",
            "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
            "      description=('Experimental release (August 1st, 2024) of Gemini 1.5 Pro, our mid-size '\n",
            "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
            "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/learnlm-1.5-pro-experimental',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='LearnLM 1.5 Pro Experimental',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
            "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1114',\n",
            "      base_model_id='',\n",
            "      version='exp-1121',\n",
            "      display_name='Gemini Experimental 1121',\n",
            "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1121',\n",
            "      base_model_id='',\n",
            "      version='exp-1121',\n",
            "      display_name='Gemini Experimental 1121',\n",
            "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='exp_1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "high_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=2.0)\n",
        ")\n",
        "\n",
        "retry_policy = {\n",
        "    'retry': retry.Retry(predicate=retry.if_transient_error, initial=10,\n",
        "                         multiplier=1.5, timeout=300)\n",
        "}\n",
        "\n",
        "for _ in range(5):\n",
        "  response = high_temp_model.generate_content('Pick a random color...(respond in single word)',\n",
        "                                              request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "rs5-XNWXnGhD",
        "outputId": "bc6c9c7e-672c-4054-e81d-288f38c64894"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maroon\n",
            " -------------------------\n",
            "Aquamarine\n",
            " -------------------------\n",
            "Purple\n",
            " -------------------------\n",
            "Aquamarine\n",
            " -------------------------\n",
            "Aquamarine\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=0.0)\n",
        ")\n",
        "\n",
        "for _ in range(5):\n",
        "  response = low_temp_model.generate_content('Pick a random color...(respond in single word)',\n",
        "                                              request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "uQ7bEsTeottH",
        "outputId": "6fa9e6a1-2716-43cd-9bc2-868874c3d794"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maroon\n",
            " -------------------------\n",
            "Maroon\n",
            " -------------------------\n",
            "Maroon\n",
            " -------------------------\n",
            "Maroon\n",
            " -------------------------\n",
            "Maroon\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "wOLhnrsppWd0",
        "outputId": "3065530a-2cb2-4568-fe64-d717bf6e637b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whiskers, a ginger tabby with a perpetual air of aloof disdain, had always considered the world beyond his windowsill to be a place of chaos and dust bunnies. But today, a strange, alluring scent drifted in – the heady perfume of adventure. \n",
            "\n",
            "He was a creature of routine, of sunbeams and naps, of his human's gentle stroking and the occasional discarded tuna can lid. But this scent, this intoxicating promise of something new, stirred something deep inside him. It was like a wild, ancient instinct, a whisper of the hunter in his blood. \n",
            "\n",
            "He watched, fascinated, as the window was flung open, letting in a rush of warm air and the playful chirping of sparrows. This was it, his chance. With a graceful leap, he slipped through the opening and landed on the sill outside.\n",
            "\n",
            "The world was a kaleidoscope of sights and sounds. The scent of blooming flowers, the buzzing of bees, the rustle of leaves in the gentle breeze. Whiskers, his eyes wide with wonder, explored the backyard, his tail twitching with anticipation. He stalked a plump robin, a fleeting shadow in the grass, his instincts taking over. He chased butterflies, their wings a blur of color, the thrill of the chase electrifying him.\n",
            "\n",
            "He met a grumpy old tomcat, his fur as gray as the ancient oak under which he sat, who warned Whiskers of the dangers of the world beyond the fence. He met a sleek black cat, eyes gleaming with mischief, who challenged Whiskers to a playful duel with a fallen branch. \n",
            "\n",
            "And then, he saw it. A small, wriggling creature, brown and furry, hiding behind a rose bush. This was the source of the alluring scent, a creature unlike any he had ever seen. It was a squirrel, and its scent promised the ultimate hunt.\n",
            "\n",
            "For hours, Whiskers stalked the squirrel, his every move calculated, his heart pounding with the thrill of the chase. Finally, he pounced, leaping onto a branch with astonishing agility. The squirrel scurried away, its tiny claws scrabbling for purchase.\n",
            "\n",
            "Whiskers, exhausted but exhilarated, watched as the squirrel disappeared into the forest. He had failed to catch his prey, but he had tasted the thrill of the hunt, the intoxicating power of the wild.\n",
            "\n",
            "As dusk settled, he heard the familiar call of his human, his name soft and soothing. He knew it was time to return. With a sigh, he leapt back onto the windowsill and slipped through the opening, the scent of adventure lingering in his fur. \n",
            "\n",
            "Back inside, curled up on his favorite cushion, he knew he would never be quite the same. He had tasted freedom, the thrill of the unknown, and the world beyond his windowsill, once a place of chaos and dust bunnies, now held a new allure, a promise of countless adventures yet to come. He closed his eyes, the gentle rhythm of his human's breathing a comforting lullaby, and dreamed of squirrels and butterflies and the intoxicating scent of the wild. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot\n",
        "Zero-shot prompts are prompts that describe the request for the model directly"
      ],
      "metadata": {
        "id": "J7QZZ2ZBrSCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5,\n",
        "    ))\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xv5XniECqtlr",
        "outputId": "a5b764aa-6f56-4020-f5ea-9298c4651100"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: **POSITIVE**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'POSITIVE'\n",
        "  NEUTRAL = 'NEUTRAL'\n",
        "  NEGATIVE = 'NEGATIVE'\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "poSlnq8hruR8",
        "outputId": "fec27261-ffdd-408d-be90-3792f1ad537c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ))\n",
        "\n",
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "\n",
        "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Aqv6Vjq8sDsL",
        "outputId": "3d789235-b03a-4f4e-8dca-77b0495c9f71"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"type\": \"normal\",\n",
            "  \"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tgSRfLHMsYlb",
        "outputId": "21d94a88-be33-4c34-8ed3-2c6472afae4d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "EBvSWLvEtAUz",
        "outputId": "77edafc5-ee33-4694-8f72-70d66a1a2491"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "71ScgKMPtcSs",
        "outputId": "584337ee-8e39-4e70-e90c-c3e3d383e695"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve this step-by-step:\n\n1. **Partner's age when you were 4:** When you were 4, your partner was 3 times your age, meaning they were 3 * 4 = 12 years old.\n\n2. **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Partner's current age:**  Since you are now 20, your partner is 20 + 8 = 28 years old.\n"
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "rKSBo_Ujtxmk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "react_chat = model.start_chat()\n",
        "\n",
        "# You will perform the Action, so generate up to, but not including, the Observation.\n",
        "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
        "\n",
        "resp = react_chat.send_message(\n",
        "    [model_instructions, example1, example2, question],\n",
        "    generation_config=config,\n",
        "    request_options=retry_policy)\n",
        "print(resp.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Cwjzym09udZv",
        "outputId": "bdf09f82-36ac-4b01-b354-60b698abbc3b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the Transformers NLP paper and identify the authors.  Then I need to determine their ages and find the youngest.  This will likely require searching for the paper and then possibly additional searches to determine author ages.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4CLFJzK-ugDu",
        "outputId": "b737ade6-1d0d-4741-bb2a-a43d9c0b1348"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1:  The provided text only lists the authors of the \"Attention is All You Need\" paper.  It does not contain any information about their ages.  Therefore, I cannot answer the question about who the youngest author was.\n",
            "\n",
            "Action 1: <finish>The provided text does not give the ages of the authors, so I cannot determine who the youngest author was.</finish>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ))\n",
        "\n",
        "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "2aLUF3kmuxrk",
        "outputId": "dac263dc-7990-43d6-98c4-3c217a2e3250"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    tools='code_execution',)\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "gZZhSSRDvcRk",
        "outputId": "7c24178f-d4fc-43d3-c43c-36fabaa09c44"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To calculate the sum of the first 14 odd prime numbers, I will first identify the first 14 odd prime numbers and then sum them.  I will use Python to help with this.  The first few odd primes are 3, 5, 7, 11, and so on.\n\n\n``` python\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\ncount = 0\nsum_of_primes = 0\nnumber = 3  # Start with the first odd prime number\n\nwhile count < 14:\n    if is_prime(number):\n        sum_of_primes += number\n        count += 1\n    number += 2  #Increment by 2 to only consider odd numbers\n\n\nprint(f\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\")\n\n\n```\n```\nThe sum of the first 14 odd prime numbers is: 326\n\n```\nTherefore, the sum of the first 14 odd prime numbers is 326.\n"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcZ5UVxUvlVE",
        "outputId": "38acd0b7-62cb-43ce-9cda-692ae9f19e68"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: \"To calculate the sum of the first 14 odd prime numbers, I will first identify the first 14 odd prime numbers and then sum them.  I will use Python to help with this.  The first few odd primes are 3, 5, 7, 11, and so on.\\n\\n\"\n",
            "\n",
            "-----\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= n:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\ncount = 0\\nsum_of_primes = 0\\nnumber = 3  # Start with the first odd prime number\\n\\nwhile count < 14:\\n    if is_prime(number):\\n        sum_of_primes += number\\n        count += 1\\n    number += 2  #Increment by 2 to only consider odd numbers\\n\\n\\nprint(f\\\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\\\")\\n\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "  output: \"The sum of the first 14 odd prime numbers is: 326\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "text: \"Therefore, the sum of the first 14 odd prime numbers is 326.\\n\"\n",
            "\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "w_vhxZ35vtX7",
        "outputId": "05c40a7c-e8a5-4875-dcd3-a1f8bd2301d8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a bash script that enhances your shell prompt to display information about your current Git repository.  It's essentially a highly customizable Git prompt theme manager.\n\nHere's why you'd use it:\n\n* **Improved Git awareness:**  Instead of a generic prompt, you get a prompt showing your current Git branch, status (e.g., changes staged, unstaged, conflicts), and potentially other information like upstream differences. This helps you quickly see the state of your Git repository without needing to run separate `git status` commands.\n\n* **Theming:**  The script supports themes (color schemes and layouts) that let you customize the appearance of your prompt.  It includes default themes and allows for custom themes.\n\n* **Cross-shell compatibility:** It attempts to work with both bash and zsh shells.\n\n* **Asynchronous operations:** It uses asynchronous commands to fetch remote changes in the background, minimizing delays in the prompt update.\n\nIn short: It makes your command line more informative and visually appealing when working with Git.  You would use it by sourcing it in your shell's configuration file (like `.bashrc` or `.zshrc`).  The script will then modify your `PS1` (prompt string) variable to include the Git information.\n"
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jZwer4ev47D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}